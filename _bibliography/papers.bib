@string {PNAS = "Proceedings of the National Academy of Sciences (PNAS)"}
% CV
@string {PAMI = "Transactions on Pattern Analysis and Machine Intelligence (TPAMI)"}
@string {IJCV = "International Journal of Computer Vision (IJCV)"}
@string {CVIU = "Computer Vision and Image Understanding (CVIU)"}
@string {TIP = "Transactions on Image Processing (TIP)"}
@string {CVPR = "The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"}
@string {ICCV = "International Conference on Computer Vision (ICCV)"}
@string {ECCV = "European Conference on Computer Vision (ECCV)"}
@string {ACCV = "Asian Conference on Computer Vision (ACCV)"}
@string {BMVC = "British Machine Vision Conference (BMVC)"}
@string {WACV = "Proceedings of Winter Conference on Applications of Computer Vision (WACV)"}
@string {ICIP = "IEEE International Conference on Image Processing (ICIP)"}
% ML
@string {JMLR = "Journal of Machine Learning Research (JMLR)"}
@string {NIPS = "Neural Information Processing Systems (NeurIPS)"}
@string {NIPSD = "Neural Information Processing Systems: Datasets and Benchmarks (NeurIPS D&B)"}
@string {ICML = "International Conference on Machine Learning (ICML)"}
@string {ICLR = "International Conference on Learning Representations (ICLR)"}
% AI
@string {JAIR = "Journal of Artificial Intelligence Research (JAIR)"}
@string {AIJ = "Artificial Intelligence"}
@string {AAAI = "AAAI Conference on Artificial Intelligence (AAAI)"}
@string {IJCAI = "International Joint Conference on Artificial Intelligence (IJCAI)"}
@string {AISTATS = "International Conference on Artificial Intelligence and Statistics (AISTATS)"}
% Robotics
@string {IJRR = "International Journal of Robotics Research (IJRR)"}
@string {TRO = "Transactions on Robotics (T-RO)"}
@string {TMECH = "Transactions on Mechatronics (TMECH)"}
@string {RA-L = "IEEE Robotics and Automation Letters (RA-L)"}
@string {RA-M = "IEEE Robotics and Automation Magazine (RA-M)"}
@string {IROS = "International Conference on Intelligent Robots and Systems (IROS)"}
@string {ICRA = "International Conference on Robotics and Automation (ICRA)"}
@string {RSS = "Robotics: Science and Systems (RSS)"}
@string {CoRL = "Conference on Robot Learning (CoRL)"}
@string {ROMAN = "International Symposium on Robot and Human Interactive Communication (RO-MAN)"}
@string {HRI = "ACM/IEEE International Conference on Human-Robot Interaction (HRI)"}
% NLP
@string {TACL = "Transactions of the Association for Computational Linguistics (TACL)"}
@string {ACL = "Annual Meeting of the Association for Computational Linguistics (ACL)"}
@string {EMNLP = "Annual Conference on Empirical Methods in Natural Language Processing (EMNLP)"}
@string {NAACL = "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)"}
@string {COLING = "International Conference on Computational Linguistics (COLING)"}
@string {CoNLL = "Conference on Computational Natural Language Learning (CoNLL)"}
@string {SIGDial = "Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDial)"}
% Psychology
@string {CogSci = "Annual Meeting of the Cognitive Science Society (CogSci)"}
% Graphics
@string {TOG = "ACM Transactions on Graphics (TOG)"}
@string {SIGGRAPH = "ACM SIGGRAPH Conference Proceedings"}
@string {TVCG = "IEEE Transactions on Visualization and Computer Graph (TVCG)"}
@string {SCA = "ACM SIGGRAPH / Eurographics Symposium on Computer Animation (SCA)"}
@string {ThreeDV = "International Conference on 3D Vision (3DV)"}
@string {CGF = "Computer Graphics Forum (CGF)"}
% HCI
@string {CHI = "ACM Conference on Human Factors in Computing Systems (CHI)"}
@string {UbiComp = "ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"}
@string {UIST = "ACM Symposium on User Interface Software and Technology (UIST)"}
% Others
@string {AAMAS = "International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"}
@string {KDD = "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)"}

@article{zhu2025mtu,
  title={Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation},
  author={Zhu, Ziyu and Wang, Xilin and Li, Yixuan and Zhang, Zhuofan and Ma, Xiaojian and Chen, Yixin and Jia, Baoxiong and Liang, Wei and Yu, Qian and Deng, Zhidong and Huang, Siyuan and Li, Qing},
  journal=ICCV,
  year={2025},
  correspondence={Deng, Zhidong and Huang, Siyuan and Li, Qing},
  arxiv={2507.04047},
  website={https://mtu3d.github.io/},
  preview={2025mtu.gif},
  selected={true},
  bibtex_show={true}
}


@article{fan2025eva,
  title={Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding},
  author={Fan, Yue and Ma, Xiaojian and Su, Rongpeng and Guo, Jun and Wu, Rujie and Chen, Xi and Li, Qing},
  journal=ICCV,
  year={2025},
  correspondence={Li, Qing},
  arxiv={2501.00358},
  website={https://embodied-videoagent.github.io/},
  preview={2025eva.png},
  selected={true},
  bibtex_show={true}
}


@article{chen2025falcon,
  title={Falcon: Fast Visuomotor Policies via Partial Denoising},
  author={Chen, Haojun and Liu, Minghao and Ma, Chengdong and Ma, Xiaojian and Ma, Zailin and Wu, Huimin and Chen, Yuanpei and Zhong, Yifan and Wang, Mingzhi and Li, Qing and Yang, Yaodong},
  journal=ICML,
  year={2025},
  arxiv={2503.00339},
  correspondence={Li, Qing and Yang, Yaodong},
  selected={true},
  bibtex_show={true}
}

@article{yu2025metascenes,
  title={MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans},
  author={Yu, Huangyue and Jia, Baoxiong and Chen, Yixin and Yang, Yandan and Su, Rongpeng and Li, Jiaxin and Li, Qing and Liang, Wei and Zhu, Song-Chun and Liu, Tengyu and Huang, Siyuan},
  journal=CVPR,
  year={2025},
  arxiv={2505.02388},
  website={https://meta-scenes.github.io/},
  preview={2025metascenes.gif},
  selected={true},
  bibtex_show={true}
}

@article{huang2025beacon3d,
  title={Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis},
  author={Huang, Jiangyong and Jia, Baoxiong and Zhu, Ziyu and Wang, Yan and Linghu, Xiongkun and Li, Qing and Zhu, Song-Chun and Huang, Siyuan},
  journal=CVPR,
  year={2025},
  arxiv={2503.22420},
  website={https://beacon-3d.github.io/},
  preview={2025beacon3d.png},
  selected={true},
  bibtex_show={true}
}

@article{2025mat,
  title={Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage}, 
  author={Gao, Zhi and Zhang, Bofei and Li, Pengxiang and Ma, Xiaojian and Yuan, Tao and Fan, Yue and Wu, Yuwei and Jia, Yunde and Zhu, Song-Chun and Li, Qing},
  journal=ICLR,
  year={2025},
  equalauthor={Li, Pengxiang and Gao, Zhi and Zhang, Bofei},
  correspondence={Wu, Yuwei and Li, Qing},
  arxiv={2412.15606},
  website={https://mat-agent.github.io/},
  preview={2025mat.png},
  selected={true},
  award={Spotlight},
  bibtex_show={true}
}

@article{2025mmke,
  title={MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge},
  author={Du, Yuntao and Jiang, Kailin and Gao, Zhi and Shi, Chenrui and Zheng, Zilong and Qi, Siyuan and Li, Qing},
  journal=ICLR,
  year={2025},
  equalauthor={Du, Yuntao and Jiang, Kailin},
  correspondence={Zheng, Zilong and Li, Qing},
  arxiv={2502.19870},
  website={https://mmke-bench-iclr.github.io/},
  preview={2025mmke.png},
  selected={true},
  bibtex_show={true}
}

@article{2024fire,      
  title = {FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models},
  author={Li, Pengxiang and Gao, Zhi and Zhang, Bofei and Yuan, Tao and Wu, Yuwei and Harandi, Mehrtash and Jia, Yunde and Zhu, Song-Chun and Li, Qing},
  journal=NIPSD,
  year = {2024},
  equalauthor={Li, Pengxiang and Gao, Zhi and Zhang, Bofei},
  correspondence={Wu, Yuwei and Li, Qing},
  arxiv={2407.11522},
  website={https://mm-fire.github.io/},
  preview={2024fire.png},
  selected={true},
  bibtex_show={true}
}

@article{2024ultraedit,
  title={UltraEdit: Instruction-based Fine-Grained Image Editing at Scale},
  author={Zhao, Haozhe and Ma, Xiaojian and Chen, Liang and Si, Shuzheng and Wu, Rujie and An, Kaikai and Yu, Peiyu and Zhang, Minjia and Li, Qing and Chang, Baobao},
  journal=NIPSD,
  year={2024},
  correspondence={Li, Qing and Chang, Baobao},
  equalauthor={Zhao, Haozhe and Ma, Xiaojian},
  arxiv={2407.05282},
  website={https://ultra-editing.github.io/},
  preview={2024ultraedit.png},
  selected={true},
  bibtex_show={true}
}

@article{2024omnijarvis,
  title={OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents},
  author={Wang, Zihao and Cai, Shaofei and Mu, Zhancun and Lin, Haowei and Zhang, Ceyao and Liu, Xuejie and Li, Qing and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
  journal=NIPS,
  year={2024},
  arxiv={2407.00114},
  website={https://omnijarvis.github.io/},
  preview={2024omnijarvis.png},
  bibtex_show={true}
}

@article{2024sg3d,
  title={Task-oriented Sequential Grounding in 3D Scenes},
  author={Zhang, Zhuofan and Zhu, Ziyu and Li, Pengxiang and Liu, Tengyu and Ma, Xiaojian and Chen, Yixin and Jia, Baoxiong and Huang, Siyuan and Li, Qing},
  journal={arXiv preprint arXiv:2408.04034},
  year={2024},
  correspondence={Li, Qing},
  arxiv={2408.04034},
  website={https://sg-3d.github.io/},
  preview={2024sg3d.png},
  selected={true},
  bibtex_show={true}
}

@article{luo2024insight,
  title={End-to-End Neuro-Symbolic Reinforcement Learning with Textual Explanations},
  author={Luo, Lirui and Zhang, Guoxi and Xu, Hongming and Yang, Yaodong and Fang, Cong and Li, Qing},
  journal=ICML,
  year={2024},
  correspondence={Fang, Cong and Li, Qing},
  arxiv={2403.12451},
  website={https://ins-rl.github.io/},
  preview={luo2024insight.png},
  selected={true},
  award={Spotlight (top 3.5%)},
  bibtex_show={true}
}

@article{zhu2024unifying,
  title={Unifying 3D Vision-Language Understanding Via Promptable Queries},
  author={Zhu, Ziyu and Zhang, Zhuofan and Ma, Xiaojian and Niu, Xuesong and Chen, Yixin and Jia, Baoxiong and Deng, Zhidong and Huang, Siyuan and Li, Qing},
  correspondence={Deng, Zhidong and Huang, Siyuan and Li, Qing},
  journal=ECCV,
  arxiv={2405.11442},
  website={https://pq3d.github.io/},
  preview={zhu2024unifying.png},
  bibtex_show={true},
  selected={true},
  year={2024}
}

@article{fan2024videoagent,
  title={VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding},
  author={Fan, Yue and Ma, Xiaojian and Wu, Rujie and Du, Yuntao and Li, Jiaqi and Gao, Zhi and Li, Qing},
  equalauthor={Fan, Yue and Ma, Xiaojian},
  correspondence={Li, Qing},
  journal=ECCV,
  arxiv={2403.11481},
  website={https://videoagent.github.io/},
  preview={fan2024videoagent.png},
  bibtex_show={true},
  selected={true},
  year={2024}
}

@article{guo2024semantic,
  title={Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting},
  author={Guo, Jun and Ma, Xiaojian and Fan, Yue and Liu, Huaping and Li, Qing},
  equalauthor={Guo, Jun and Ma, Xiaojian},
  correspondence={Liu, Huaping and Li, Qing},
  journal={arXiv preprint arXiv:2403.15624},
  arxiv={2403.15624},
  website={https://semantic-gaussians.github.io/},
  preview={guo2024semantic.png},
  bibtex_show={true},
  year={2024}
}

@article{xin2024parameter,
  title={Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey},
  author={Xin, Yi and Luo, Siqi and Zhou, Haodi and Du, Junlong and Liu, Xiaohong and Fan, Yue and Li, Qing and Du, Yuntao},
  journal={arXiv preprint arXiv:2402.02242},
  arxiv={2402.02242},
  preview={xin2024parameter.png},
  bibtex_show={true},
  year={2024}
}

@article{jia2024sceneverse,
  title={SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding},
  author={Jia, Baoxiong and Chen, Yixin and Yu, Huangyue and Wang, Yan and Niu, Xuesong and Liu, Tengyu and Li, Qing and Huang, Siyuan},
  equalauthor={Jia, Baoxiong and Chen, Yixin},
  journal=ECCV,
  arxiv={2401.09340},
  website={https://scene-verse.github.io/},
  preview={jia2024sceneverse.png},
  bibtex_show={true},
  selected={true},
  year={2024}
}

@article{gao2024clova,
  title={CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update},
  author={Gao, Zhi and Du, Yuntao and Zhang, Xintong and Ma, Xiaojian and Han, Wenjuan and Zhu, Song-Chun and Li, Qing},
  correspondence={Li, Qing},
  journal=CVPR,
  arxiv={2312.10908},
  website={https://clova-tool.github.io/},
  preview={gao2024clova.png},
  bibtex_show={true},
  selected={true},
  year={2024}
}

@article{huang2024embodied,
  title={An Embodied Generalist Agent in 3D World},
  author={Huang, Jiangyong and Yong, Silong and Ma, Xiaojian and Linghu, Xiongkun and Li, Puhao and Wang, Yan and Li, Qing and Zhu, Song-Chun and Jia, Baoxiong and Huang, Siyuan},
  equalauthor={Huang, Jiangyong and Yong, Silong and Ma, Xiaojian and Linghu, Xiongkun},
  journal=ICML,
  arxiv={2311.12871},
  website={https://embodied-generalist.github.io/},
  preview={huang2024embodied.png},
  bibtex_show={true},
  selected={true},
  year={2024}
}

@article{li2024nsr,
  title={Neural-Symbolic Recursive Machine for Systematic Generalization},
  author={Li, Qing and Zhu, Yixin and Liang, Yitao and Wu, Ying Nian and Zhu, Song-Chun and Huang, Siyuan},
  journal=ICLR,
  arxiv={2210.01603},
  website={https://liqing-ustc.github.io/NSR/},
  preview={li2024nsr.png},
  bibtex_show={true},
  selected={true},
  year={2024}
}

@article{wu2024bongard,
  title={Bongard-OpenWorld: Few-Shot Reasoning for Free-Form Visual Concepts in the Real World},
  author={Wu, Rujie and Ma, Xiaojian and Zhang, Zhenliang and Wang, Wei and Li, Qing and Zhu, Song-Chun and Wang, Yizhou},
  equalauthor={Wu, Rujie and Ma, Xiaojian},
  correspondence={Li, Qing and Wang, Wei},
  journal=ICLR,
  arxiv={2310.10207},
  website={https://joyjayng.github.io/Bongard-OpenWorld.github.io/},
  preview={wu2024bongard.png},
  bibtex_show={true},
  selected={true},
  year={2024}
}

@article{qin2023learning,
  title={Learning Non-Markovian Decision-Making from State-Only Sequences},
  author={Qin, Aoyang and Gao, Feng and Li, Qing and Zhu, Song-Chun and Xie, Sirui},
  journal=NIPS,
  arxiv={2306.15156},
  code={https://github.com/qayqaq/LanMDP},
  preview={qin2023learning.png},
  bibtex_show={true},
  selected={true},
  year={2023}
}

@article{li2023hint,
  title={A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics},
  author={Li, Qing and Huang, Siyuan and Hong, Yining and Zhu, Yixin and Wu, Ying Nian and Zhu, Song-Chun},
  journal=ICLR,
  award={Notable-top-25%},
  arxiv={2103.01403},
  website={https://liqing-ustc.github.io/HINT/},
  preview={li2023hint.png},
  bibtex_show={true},
  selected={true},
  year={2023}
}

@article{zhu2023vista,
  title={3D-VisTA: Pre-Trained Transformer for 3D Vision and Text Alignment},
  author={Zhu, Ziyu and Ma, Xiaojian and Chen, Yixin and Deng, Zhidong and Huang, Siyuan and Li, Qing},
  correspondence={Deng, Zhidong and Huang, Siyuan and Li, Qing},
  journal=ICCV,
  arxiv={2308.04352},
  website={https://3d-vista.github.io/},
  preview={zhu2023vista.png},
  bibtex_show={true},
  selected={true},
  year={2023}
}

@article{ma2023sqa3d,
  title={SQA3D: Situated Question Answering in 3D Scenes},
  author={Ma, Xiaojian and Yong, Silong and Zheng, Zilong and Li, Qing and Liang, Yitao and Zhu, Song-Chun and Huang, Siyuan},
  equalauthor={Ma, Xiaojian and Yong, Silong},
  journal=ICLR,
  arxiv={2210.07474},
  website={https://sqa3d.github.io/},
  preview={ma2023sqa3d.png},
  bibtex_show={true},
  selected={true},
  year={2023}
}

@article{li2022close,
  title={Close the Loop of Neural Perception, Grammar Parsing, and Symbolic Reasoning},
  author={Li, Qing},
  journal={University of California, Los Angeles},
  bibtex_show={true},
  year={2022}
}

@article{li2021unified,
  title={Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text},
  author={Li, Qing and Gong, Boqing and Cui, Yin and Kondratyuk, Dan and Du, Xianzhi and Yang, Ming-Hsuan and Brown, Matthew},
  journal={arXiv preprint arXiv:2112.07074},
  arxiv={2112.07074},
  preview={li2021unified.png},
  bibtex_show={true},
  year={2021}
}

@article{hong2021smart,
  title={SMART: A Situation Model for Algebra Story Problems via Attributed Grammar},
  author={Hong, Yining and Li, Qing and Gong, Ran and Ciao, Daniel and Huang, Siyuan and Zhu, Song-Chun},
  journal=AAAI,
  arxiv={2012.14011},
  preview={hong2021smart.png},
  bibtex_show={true},
  selected={true},
  year={2021}
}

@article{hong2021learning,
  title={Learning by Fixing: Solving Math Word Problems with Weak Supervision},
  author={Hong, Yining and Li, Qing and Ciao, Daniel and Huang, Siyuan and Zhu, Song-Chun},
  journal=AAAI,
  arxiv={2012.10582},
  preview={hong2021learning.png},
  bibtex_show={true},
  selected={true},
  year={2021}
}

@article{chen2021yourefit,
  title={YouRefIt: Embodied Reference Understanding with Language and Gesture},
  author={Chen, Yixin and Li, Qing and Kong, Deqian and Kei, Yik Lun and Zhu, Song-Chun and Gao, Tao and Zhu, Yixin and Huang, Siyuan},
  journal=ICCV,
  award={Oral},
  arxiv={2109.03413},
  preview={chen2021yourefit.png},
  bibtex_show={true},
  selected={true},
  year={2021}
}

@article{hong2021vlgrammar,
  title={VLGrammar: Grounded Grammar Induction of Vision and Language},
  author={Hong, Yining and Li, Qing and Zhu, Song-Chun and Huang, Siyuan},
  journal=ICCV,
  arxiv={2103.12975},
  preview={hong2021vlgrammar.png},
  bibtex_show={true},
  selected={true},
  year={2021}
}

@article{li2020competence,
  title={A Competence-Aware Curriculum for Visual Concepts Learning Via Question Answering},
  author={Li, Qing and Huang, Siyuan and Hong, Yining and Zhu, Song-Chun},
  journal=ECCV,
  award={Oral},
  arxiv={2007.01499},
  website={https://liqing-ustc.github.io/CL-mIRT/},
  preview={li2020competence.png},
  bibtex_show={true},
  selected={true},
  year={2020}
}

@article{li2020ngs,
  title={Closed Loop Neural-Symbolic Learning Via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning},
  author={Li, Qing and Huang, Siyuan and Hong, Yining and Chen, Yixin and Wu, Ying Nian and Zhu, Song-Chun},
  journal=ICML,
  award={Best Paper in ICML Workshop},
  arxiv={2006.06649},
  website={https://liqing-ustc.github.io/NGS/},
  preview={li2020ngs.png},
  bibtex_show={true},
  selected={true},
  year={2020}
}

@article{bhattacharya2019visual,
  title={Why Does a Visual Question Have Different Answers?},
  author={Bhattacharya, Nilavra and Li, Qing and Gurari, Danna},
  journal=ICCV,
  arxiv={1908.04342},
  website={https://vizwiz.org/},
  preview={bhattacharya2019visual.png},
  bibtex_show={true},
  selected={true},
  year={2019}
}

@article{gurari2019vizwizpriv,
  title={VizWiz-Priv: A Dataset for Recognizing the Presence and Purpose of Private Visual Information in Images Taken by Blind People},
  author={Gurari, Danna and Li, Qing and Lin, Chi and Zhao, Yinan and Guo, Anhong and Stangl, Abigale and Bigham, Jeffrey P},
  journal=CVPR,
  website={https://vizwiz.org/},
  preview={gurari2019vizwizpriv.png},
  bibtex_show={true},
  selected={true},
  year={2019}
}

@article{li2018tell,
  title={Tell-and-Answer: Towards Explainable Visual Question Answering Using Attributes and Captions},
  author={Li, Qing and Fu, Jianlong and Yu, Dongfei and Mei, Tao and Luo, Jiebo},
  journal=EMNLP,
  award={Oral},
  arxiv={1801.09041},
  preview={li2018tell.png},
  bibtex_show={true},
  selected={true},
  year={2018}
}

@article{li2018vqa,
  title={VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions},
  author={Li, Qing and Tao, Qingyi and Joty, Shafiq and Cai, Jianfei and Luo, Jiebo},
  journal=ECCV,
  arxiv={1803.07464},
  preview={li2018vqa.png},
  bibtex_show={true},
  selected={true},
  year={2018}
}

@article{gurari2018vizwiz,
  title={VizWiz Grand Challenge: Answering Visual Questions from Blind People},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  journal=CVPR,
  award={Spotlight},
  website={https://vizwiz.org/},
  arxiv={1802.08218},
  preview={gurari2018vizwiz.png},
  bibtex_show={true},
  selected={true},
  year={2018}
}

@article{li2017learning,
  title={Learning Hierarchical Video Representation for Action Recognition},
  author={Li, Qing and Qiu, Zhaofan and Yao, Ting and Mei, Tao and Rui, Yong and Luo, Jiebo},
  journal={International Journal of Multimedia Information Retrieval},
  volume={6},
  pages={85-98},
  preview={li2017learning.png},
  bibtex_show={true},
  year={2017}
}

@article{li2016action,
  title={Action Recognition by Learning Deep Multi-Granular Spatio-Temporal Video Representation},
  author={Li, Qing and Qiu, Zhaofan and Yao, Ting and Mei, Tao and Rui, Yong and Luo, Jiebo},
  journal={International Conference on Multimedia Retrieval},
  award={Best Paper Finalist},
  preview={li2016action.png},
  bibtex_show={true},
  selected={true},
  year={2016}
}
